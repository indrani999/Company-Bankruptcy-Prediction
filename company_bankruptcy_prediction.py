# -*- coding: utf-8 -*-
"""Company Bankruptcy Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r8e6_8uVMqaWfwl4H6oVeJRjvYT7MXpE

# Context

The data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

df = pd.read_csv('/content/data.csv')

df.head(7)

# feature names as a list
col = df.columns
print(col)

df.describe

df.shape

# finding null values
print(df.isnull().values.any())

# get the number of missing data points per column
missing_value_count = (df.isnull().sum())
print(missing_value_count[missing_value_count > 0])
# percent of data that is missing
total_cells = np.product(df.shape)
total_missing_value = missing_value_count.sum()
print(total_missing_value / total_cells * 100)
print('Total number of our cells is :',total_cells)
print('Total number of our missing value is :',total_missing_value)
######### 2 üsulla göstərmək istədim.

#using a supervised machine learning models, correlation and variations to judge the importance of each feature, and keeps only the most important ones
#Filter by variation
var=df.var()
print(var)

fig_dimns=(10,6)
fig, ax= plt.subplots(figsize=fig_dimns)
sns.heatmap(df.corr(), ax=ax)
plt.show()

# Separate features (X) and labels (y)
X = df.drop('Bankrupt?', axis=1)  # The drop method is used to remove a specified column ('Bankrupt?') from the DataFrame.
#The axis=1 parameter indicates that we are dropping a column
y = df['Bankrupt?']
# This separation is often done to prepare the data for training a model,
#where X is used to train the model, and y is used to evaluate its performance.

X = df.iloc[:,1:].values
y = df.iloc[:,0].values.reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=0)

clsf=RandomForestClassifier(n_estimators=100,random_state=0)
clsf.fit(X_train, y_train)

y_pred = clsf.predict(X_test)
#obtain predictions

from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.3f}")

print("Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test,clsf.predict(X_test))
sns.heatmap(cm,annot=True,fmt="",cmap='Blues')

"""DESCRIPTIVE ANALYSIS"""

#analyzing target variable
sns.countplot(data=df, x='Bankrupt?', palette='bwr',)
plt.xlabel('Target')
plt.ylabel('Count of target')
plt.title('Target variable countplot')
plt.show()

df.groupby('Bankrupt?').size()

# for seeing data ranges for understanding data and finding outliers of data
plt.figure(figsize=(20,20))
sns.set_style('whitegrid')
sns.set_palette('bwr')
sns.boxplot(data = df.drop(['Bankrupt?'], axis=1), orient='h', color='blue')
plt.title('Data range')
plt.show()

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import KFold, StratifiedKFold

print("Financially Stable:", round(df['Bankrupt?'].value_counts()[0] / len(df) * 100, 2), '% of the dataset')
print("Financially Unstable:", round(df['Bankrupt?'].value_counts()[1] / len(df) * 100,2),'% of the dataset')

X = df.drop('Bankrupt?', axis = 1)
y = df['Bankrupt?']

sss = StratifiedKFold(n_splits = 5, random_state = None, shuffle = False)

for train_index, test_index in sss.split(X,y):
    print("\n Train", train_index, "Test", test_index)
    org_Xtrain, org_Xtest = X.iloc[train_index], X.iloc[test_index]
    org_ytrain, org_ytest = y.iloc[train_index], y.iloc[test_index]

## Lets shuffle the data before creating the subsamples

xdf = df.sample(frac = 1)

## amount of Financially unstable data is 220
# sdf = Financially stable
# ndf = Financially unstable

sdf = df.loc[xdf['Bankrupt?'] == 0][:220]
ndf = df.loc[xdf['Bankrupt?']==1]

normal_distributed_df = pd.concat([sdf, ndf])

# Shuffling again

nxdf = normal_distributed_df.sample(frac = 1, random_state = 42)

nxdf.head()

## Checking new dataframe

print("Distribution of the Classes in the subsample dataset")
print(nxdf['Bankrupt?'].value_counts() / len(nxdf))

sns.countplot( data = nxdf)
plt.title("Equally Distributed Class", fontsize = 14)
plt.show()

"""HANDLING OUTLIERS"""

pip install pandas scipy

from scipy.stats import zscore

# Calculate z-scores for all columns

z_scores = zscore(nxdf.select_dtypes(include=['number']))

# Set a threshold for identifying outliers (e.g., 3 standard deviations)
threshold = 3

# Identify outliers across all columns
outliers = (abs(z_scores) > threshold).any(axis=1)

# Print indices or values of outliers
print("Indices of outliers:", outliers.index[outliers].tolist())
print("Values of outliers:", nxdf[outliers])

#Remove outliers because they dont affect my analysis
# also i used random forest classifier which is less senstive for outliers
nxdf_cleaned = nxdf[outliers]

"""LOGISTIC REGRESSION"""

y = df.iloc[:,:1]
y

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Flatten the y_train_resampled array
y_train_1d = np.ravel(y_train)
#multi-dimensional array y_train into a one-dimensional array

"""Normalization with scaller"""

from sklearn.preprocessing import MinMaxScaler

MM = MinMaxScaler()

X_train_MM = MM.fit_transform(X_train)
#The MinMaxScaler is a preprocessing method that scales features by transforming them to a specific range.
#It is commonly used in machine learning to standardize the range of independent variables.

X_test_MM = MM.transform(X_test)

LR = LogisticRegression(max_iter=100,solver='liblinear')

LR.fit(X_train_MM,y_train_1d)
#fitting (training) a logistic regression model (LR) using the training data after MinMax scaling (X_train_MM)
#and the corresponding one-dimensional target variable (y_train_1d)

y_hat = LR.predict(X_test_MM)

from sklearn.metrics import accuracy_score, confusion_matrix , classification_report

print(accuracy_score(y_test,y_hat))

print(classification_report(y_test,y_hat))

cm = confusion_matrix(y_test,LR.predict(X_test_MM))
sns.heatmap(cm,annot=True,fmt="d",cmap = "Blues")

"""SVM(support vector machines)"""

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import f1_score, recall_score, accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC

##Split train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y, random_state=15)

#Fine tune SVM parameters
for weight in [1, 3, 5, 6, 10, 50, 100]:
    #Build SVC model
    pipe_svc = Pipeline(steps=[('scale', StandardScaler()), ('SVC', SVC(class_weight={0:1, 1:weight}))])
    score = cross_val_score(pipe_svc, X_train, y_train, n_jobs=4, scoring = 'f1', cv=5).mean()
    print('Mean F1 cross-val-score for model with weight %i is %.2f' % (weight, score))

#Train and evaluate on test set
pipe_svc = Pipeline(steps=[('scale', StandardScaler()), ('SVC', SVC(class_weight={0:1, 1:6}, random_state=10))])
pipe_svc.fit(X_train, y_train)
y_pred = pipe_svc.predict(X_test)
test_f1 = f1_score(y_test, y_pred)
print('Test f1_score is', test_f1)

test_recall =  recall_score(y_test, y_pred)
print('Test recall score is ', test_recall)
test_accuracy = accuracy_score(y_test, y_pred)
print('Test accuarcy score is', test_accuracy)

plt.figure(figsize=(12,8))
sns.heatmap(y_pred.reshape(2046,1), cmap='bwr')
plt.title('Visualize predicted bankrupt company')
plt.show()

##Memorizing support vector:
s_vector = pipe_svc[1].support_vectors_
vector_index = pipe_svc[1].support_

##Create a new column in the dataset indicate whether the sample is a support vector or not:
S_vector = []
for i in range(0, len(X_train)):
    if i in vector_index:
        S_vector.append(1)
    else:
        S_vector.append(0)
X_train['Is Vector'] = S_vector

"""
We successfully created three models with an accuracy of 97%, using only 7 features. This not only saved a significant amount of time (reducing from 5.47 seconds to just 0.5 seconds) but also allowed us to better explain how a company might go bankrupt or not. In simpler terms, we found that companies with high interest-bearing debt interest rates, total debt to total net worth, and fixed assets turnover frequency are more likely to go bankrupt. Additionally, those with low cash to total assets and low equity to liability are also at higher risk. Moreover, companies with lower net profit before tax/paid-in capital, persistent EPS in the last four seasons, and net value per share (A) are more prone to bankruptcy."""

